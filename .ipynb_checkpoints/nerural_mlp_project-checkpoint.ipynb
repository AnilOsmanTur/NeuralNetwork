{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# neural network\n",
    "# - multilayer perceptron implementation project\n",
    "\n",
    "## Difined classes\n",
    " - layer class;\n",
    "   - It has forward pass and back-propagation implementations for a layer\n",
    "   - To you use layer class multiLayerPerceptron class is needed\n",
    " - multiLayerPerceptron class;\n",
    "   - It has the needed functions and some missing parts of layer class and\n",
    "     it complates the layer class. \n",
    "\n",
    " - AnÄ±l Osman TUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "random.seed(216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network generalizied layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a hidden layer don't give any error function\n",
    "# for output layer give a error function\n",
    "class layer:\n",
    "    def __init__(self, n_neuron, inputs, desired_outputs=None, activation=None,\n",
    "                 d_activation=None, error=None, d_error=None):\n",
    "        self.inputs = np.ones((inputs.shape[0], inputs.shape[1] + 1))  # ones for bias\n",
    "        self.inputs[:, :-1] = inputs  # ones for biases added at last of every row of input\n",
    "        # for arranging the weight matrix we need \n",
    "        self.n_neuron = n_neuron\n",
    "        self.n_input = inputs.shape[1] + 1\n",
    "        # if we give a cost functin neuron will be a output layer otherwise it will be a hidden layer\n",
    "        if error != None:\n",
    "            self.error_f = error\n",
    "            self.labels = desired_outputs\n",
    "            self.layer_type = \"output\"\n",
    "            self.costs = np.ones((inputs.shape[0], n_neuron))\n",
    "            self.errors = np.ones((inputs.shape[0], n_neuron))\n",
    "            self.total_errors = np.zeros((inputs.shape[0], 1)) # for every input group there will be error sum\n",
    "        else:\n",
    "            if desired_outputs != None:\n",
    "                self.labels = desired_outputs\n",
    "                self.layer_type = \"output\"\n",
    "                self.costs = np.zeros((inputs.shape[0], n_neuron))\n",
    "                self.errors = np.zeros((inputs.shape[0], n_neuron))\n",
    "                self.total_errors = np.zeros((inputs.shape[0], 1)) # for every input group there will be error sum\n",
    "            else:\n",
    "                self.layer_type = \"hidden\" \n",
    "            \n",
    "        self.W = self.initWeight((n_neuron,   self.n_input))  # weights with bias\n",
    "        # activating neurons\n",
    "        self.v_net = np.dot(self.inputs, self.W.T)\n",
    "        if activation == None:\n",
    "            self.activation = None\n",
    "            self.out = self.v_net\n",
    "        else:\n",
    "            self.activation = activation\n",
    "            self.out = activation(self.v_net)  # V_out = sigmoid ( W*x + b )\n",
    "        self.d_activation = d_activation\n",
    "        self.grad = np.zeros((self.inputs.shape[0], self.n_neuron))  # local gradient\n",
    "        self.grad_down = np.zeros((self.inputs.shape[0], self.n_neuron)) # gradient for before layer\n",
    "        # for output neurons cost calcualtions \n",
    "        if self.layer_type == \"output\":\n",
    "            self.cost(self.labels, self.out)\n",
    "            if self.error_f != None:\n",
    "                self.errors = self.error_f( self.costs )\n",
    "            else:\n",
    "                self.errors = self.costs\n",
    "            self.total_errors = np.array([np.mean(self.errors, axis=1)])\n",
    "        \n",
    "        \n",
    "        # cost fonction : desired - actvation output ( d - sigmoid ( W*x + b ) )\n",
    "    def cost(self, desired, out):\n",
    "        self.costs = desired - out\n",
    "        \n",
    "        # for first weight initializaiton random weights\n",
    "    def initWeight(self, shape):\n",
    "        W_values = np.asarray(np.random.uniform(\n",
    "                    low=-np.sqrt(6. / (shape[0] + shape[1]))/100,\n",
    "                    high=np.sqrt(6. / (shape[0] + shape[1]))/100,\n",
    "                    size=shape))\n",
    "        return W_values\n",
    "        \n",
    "    def fprop(self, inputs, desired_outputs=None):\n",
    "        self.inputs = np.ones((inputs.shape[0], inputs.shape[1] + 1))  # ones for bias\n",
    "        self.inputs[:, :-1] = inputs  # ones for biases added at last of every row of input\n",
    "        self.v_net = np.dot(self.inputs, self.W.T) # V_net = W*x + b\n",
    "        self.out = self.activation(self.v_net)  # V_out = sigmoid ( W*x + b )\n",
    "        # if layer is output layer we need to calculate errors\n",
    "        if self.layer_type == \"output\":\n",
    "            if desired_outputs != None:\n",
    "                self.labels = desired_outputs\n",
    "                self.cost(self.labels, self.out)\n",
    "                if self.error_f != None:\n",
    "                    self.errors = self.error_f( self.costs )\n",
    "                else:\n",
    "                    self.errors = self.costs\n",
    "                self.total_errors = np.array([np.mean(self.errors, axis=1)])\n",
    "        return self.out\n",
    "\n",
    "    # weight update function\n",
    "    def update_w(self, lr):\n",
    "        self.W = self.W + lr * np.dot(self.grad.T, self.inputs) # not sure about the sign here !!!!!\n",
    "\n",
    "    # general back propagation for hidden and output layers \n",
    "    def bprop(self, backInput=None, lr=0.1): # if its a hidden layer we have to put back input\n",
    "        if self.layer_type == \"output\":\n",
    "            grad_above = self.costs\n",
    "        else:\n",
    "            grad_above = backInput # if its a hidden layer we have to take # not sure about \"-\" here\n",
    "        self.grad = -self.d_activation(self.out) * grad_above\n",
    "        self.grad_down = np.dot(self.grad, self.W[:,:-1])\n",
    "        self.update_w(lr)\n",
    "        return self.grad_down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implementing the network\n",
    "\n",
    "class multiLayerPerseptron():\n",
    "    def __init__(self, inputs, labels, lr=0.1):\n",
    "        if inputs.shape[0] == labels.shape[0]:\n",
    "            self.inputs = inputs\n",
    "            self.labels = labels\n",
    "            self.inCounts = self.inputs.shape[0]\n",
    "            self.lr = lr\n",
    "            \n",
    "            self.first_inputs = np.array( [self.inputs[0]] )\n",
    "            self.first_labels = np.array( [self.labels[0]] ) \n",
    "        \n",
    "            # layers\n",
    "            self.layers = []\n",
    "            self.layer = None\n",
    "            self.layer_count = 0\n",
    "            self.adding_end = False\n",
    "            \n",
    "        else:\n",
    "            print \"ERROR: inputs and labes not equal!\"\n",
    "        \n",
    "    def add_layer(self, n_neuron, layer_type='hidden'):\n",
    "        if layer_type != 'output' and layer_type != 'hidden':\n",
    "            print \"Unknown layer type. Please chose from hidden or output type.\"\n",
    "        elif not self.adding_end:\n",
    "            if self.layer_count == 0 and layer_type == 'output':\n",
    "                print \"You didn't add any hidden layer.This will a perseptron.\\nYou can't add more layer. You can start training.\"\n",
    "                self.layer = layer( n_neuron, self.first_inputs, desired_outputs=self.first_labels,\n",
    "                              activation=self.sigmoid, d_activation=self.d_sigmoid, error=self.squared_error)\n",
    "                self.layers.append(self.layer)\n",
    "                self.adding_end = True\n",
    "                self.layer_count += 1\n",
    "            elif layer_type == 'output':\n",
    "                print \"Output layer will close adding you can't add more layer.\\nYou can start training.\"\n",
    "                self.layer = layer( n_neuron, self.layers[self.layer_count-1].out, desired_outputs=self.first_labels,\n",
    "                              activation=self.sigmoid, d_activation=self.d_sigmoid, error=self.squared_error)\n",
    "                self.layers.append(self.layer)\n",
    "                self.adding_end = True\n",
    "                self.layer_count += 1\n",
    "            elif self.layer_count == 0:\n",
    "                self.layer = layer( n_neuron, self.first_inputs, activation=self.sigmoid, d_activation=self.d_sigmoid)\n",
    "                self.layers.append(self.layer)\n",
    "                self.layer_count += 1\n",
    "            else:\n",
    "                self.layer = layer( n_neuron, self.layers[self.layer_count-1].out, activation=self.sigmoid, d_activation=self.d_sigmoid)\n",
    "                self.layers.append(self.layer)\n",
    "                self.layer_count += 1\n",
    "            # layer adding finished\n",
    "            if self.adding_end:\n",
    "                print \"first weigths:\"\n",
    "                self.printWeights()\n",
    "                # seeing the outputs\n",
    "                print \"first outputs:\"\n",
    "                self.printOutputs()\n",
    "        else:\n",
    "            print \"Layer couldn't created.\"\n",
    "            print \"Output layer had closed adding you can't add more layer.\\nYou can start training.\"\n",
    "        \n",
    "        \n",
    "    def forwardPass(self, inputs, labels=None, show=False):\n",
    "        for i in xrange(len(self.layers)):\n",
    "            if i == 0:\n",
    "                self.layers[i].fprop(inputs)\n",
    "            elif i == len(self.layers) - 1:\n",
    "                self.layers[i].fprop(self.layers[i-1].out, desired_outputs=labels)\n",
    "            else:\n",
    "                self.layers[i].fprop(self.layers[i-1].out)\n",
    "        if show:\n",
    "            # seeing the outputs\n",
    "            print \"outputs\"\n",
    "            self.printOutputs()\n",
    "        \n",
    "    def backPropagate(self, show=False):\n",
    "        for i in xrange(len(self.layers)-1, -1, -1):\n",
    "            if i == len(self.layers)-1:\n",
    "                self.layers[i].bprop(self.lr)\n",
    "            else:\n",
    "                self.layers[i].bprop( backInput=self.layers[i+1].grad_down, lr=self.lr )\n",
    "        if show:\n",
    "            #seeing the result of back propagation\n",
    "            print \"backpropagation done\"\n",
    "            self.printWeights()\n",
    "        \n",
    "    # train network for one epoch\n",
    "    def train(self, show=False):\n",
    "        # training for one epoch\n",
    "        for inputCount in range(0 ,self.inCounts):\n",
    "            self.forwardPass( np.array( [self.inputs[inputCount]] ), \n",
    "                              labels=np.array( [self.labels[inputCount]] ), show=show)\n",
    "            self.backPropagate(show=show)\n",
    "                \n",
    "    # training network with training set\n",
    "    def fit(self, epochs, show=False):\n",
    "        print \"Training started ...\"\n",
    "        for epoch in range(epochs):\n",
    "            s = \"Epoch :\" +  str(epoch)\n",
    "            print s,\n",
    "            self.train(show=show)\n",
    "            print '\\r' * len(s),\n",
    "        print \"Trainging finalized.\"\n",
    "        print \"After trainging new weights: \"\n",
    "        self.printWeights()\n",
    "    \n",
    "    # making predictions\n",
    "    def predict(self, x, show=False):\n",
    "        predictions = []\n",
    "        count = x.shape[0]\n",
    "        print \"predicting ...\"\n",
    "        for inputCount in range(0 ,count):\n",
    "            self.forwardPass( np.array( [x[inputCount]] ) )\n",
    "            predictions.append(self.layers[-1].out)\n",
    "        print \"done !\"\n",
    "        if show:\n",
    "            print \"predictions: \"\n",
    "            print \"inputs: \\n\",  x\n",
    "            print \"predictions: \\n\", predictions\n",
    "        return predictions\n",
    "    \n",
    "    # validating network with test set\n",
    "    def validate(self, x, y, show=False):\n",
    "        accuracy = 0.0\n",
    "        falseCount = 0\n",
    "        trueCount = 0\n",
    "        allCount = x.shape[0]\n",
    "        print \"validating ...\"\n",
    "        for inputCount in range(0 ,allCount):\n",
    "            self.forwardPass( np.array([x[inputCount]]) , labels=np.array([y[inputCount]]) )\n",
    "            trueAnswer = np.argmax( y[inputCount] )\n",
    "            if show:\n",
    "                print \"Test index :\", inputCount\n",
    "                print \"Given answer : \",self.layers[-1].out\n",
    "                print \"True answer : \", y[inputCount]\n",
    "            if np.argmax(self.layers[-1].out) == trueAnswer:\n",
    "                error = self.layers[-1].costs[:,trueAnswer]\n",
    "                if show:\n",
    "                    print \"Error: \", error\n",
    "                if error < 0.5 and error > -0.5:\n",
    "                    trueCount += 1\n",
    "                else:\n",
    "                    if show:\n",
    "                        print \"not sure but true.\"\n",
    "                    trueCount += 1\n",
    "            else:\n",
    "                falseCount += 1\n",
    "        print \"from \",allCount,\" data, \",trueCount,\" true, \",falseCount, \" false prediction.\"\n",
    "        accuracy = float(trueCount) / allCount\n",
    "        print(\"accuracy: {:.2f}%\".format((accuracy) * 100))\n",
    "        print \"done !\"\n",
    "            \n",
    "    # print layer types\n",
    "    def printNet(self):\n",
    "        print \"Network Structure:\"\n",
    "        for i in range(len(self.layers)):\n",
    "            print self.layers[i].layer_type, \"layer with \", self.layers[i].n_neuron, \"neuron\"\n",
    "\n",
    "    \n",
    "    # printing weights\n",
    "    def printWeights(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print self.layers[i].layer_type, \" layer \", i, \" weights: \\n\",self.layers[i].W\n",
    "        \n",
    "    def printOutputs(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print self.layers[i].layer_type, \" layer \", i, \" output: \\n\",self.layers[i].out\n",
    "        \n",
    "    def printErrors(self):\n",
    "        print \"output layer costs: \\n\", self.layers[-1].costs\n",
    "        print \"output layer errors: \\n\", self.layers[-1].errors\n",
    "        print \"output layer error average: \\n\", self.layers[-1].total_errors\n",
    "    \n",
    "    def plot_weights(self, l):\n",
    "        plt.title('weights')\n",
    "        plt.imshow(l.W, interpolation='nearest')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_layers(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.plot_weights(self.layers[i])\n",
    "            \n",
    "    # activation function\n",
    "    def sigmoid( self, x ):\n",
    "        return 1 / ( 1 + np.exp(x) )\n",
    "\n",
    "    def d_sigmoid( self, out ):\n",
    "        return out * ( 1 - out )\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def d_relu(self, x):\n",
    "        return (x > 0)\n",
    "    \n",
    "    def softmax( self, x ):\n",
    "        exps = np.exp(x)\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    def d_softmax( self, out ):\n",
    "        pass\n",
    "    \n",
    "    # loss functions\n",
    "    def cross_entropy( self, out, t ):\n",
    "        return -np.sum( t * np.log(out) + ( 1 - t ) * np.log(1 - out) )\n",
    "    \n",
    "    def d_cross_entropy( self, out, t ):\n",
    "        pass\n",
    "    \n",
    "    def squared_error( self, cost ): # desired value and y as output of neurons\n",
    "        return  np.square( cost ) / 2\n",
    "\n",
    "    def d_squared_error( self, cost ): # a bit meaningless to use\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_file = \"/home/***/datasets/iris_plants/iris.mat\"\n",
    "\n",
    "def loadData(fileName):\n",
    "    \n",
    "    class_count = -1\n",
    "    same_class_dataCount = 0\n",
    "    data_class = \"\"\n",
    "    last_data_class = \"\"\n",
    "    input_model = []\n",
    "    classes = []\n",
    "    \n",
    "    fileI = open(fileName, 'r')\n",
    "    \n",
    "    input_s = fileI.readline()[:-1] # reading without readline\n",
    "    input_model = input_s.split(',')\n",
    "    \n",
    "    data = []\n",
    "    while input_s != '':\n",
    "        for i in range(4):\n",
    "            input_model[i] = float(input_model[i])    \n",
    "        data_class = input_model[4]\n",
    "        if data_class == last_data_class:\n",
    "            same_class_dataCount += 1\n",
    "        else:\n",
    "            class_count += 1\n",
    "            classes.append(data_class)\n",
    "        input_model[4] = class_count\n",
    "        last_data_class = data_class\n",
    "        \n",
    "        data.append(input_model)\n",
    "        input_s = fileI.readline()[:-1]\n",
    "        input_model = input_s.split(',')\n",
    "        \n",
    "    fileI.close()\n",
    "    return data, classes\n",
    "data, classes = loadData(dataset_file)\n",
    "\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization and label creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "max_v = np.amax(data[:,:-1], axis=0)\n",
    "min_v = np.amin(data[:,:-1], axis=0)\n",
    "\n",
    "normalized_data = (data[:,:-1] - min_v) / (max_v - min_v)\n",
    "labels = data[:,-1]\n",
    "\n",
    "cato_labels = np.zeros((len(labels),len(classes)))\n",
    "for i, label in enumerate(labels):\n",
    "    cato_labels[i][label] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test and train split\n",
    "test_data = normalized_data[:20]\n",
    "test_labels = cato_labels[:20]\n",
    "\n",
    "train_data = normalized_data[20:]\n",
    "train_labels = cato_labels[20:]\n",
    "#print train_data[:10], \"\\n\", train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer will close adding you can't add more layer.\n",
      "You can start training.\n",
      "first weigths:\n",
      "hidden  layer  0  weights: \n",
      "[[  3.09804999e-03   6.13563704e-03  -6.94561343e-04  -4.04822929e-03\n",
      "   -2.02633220e-03]\n",
      " [ -4.37241186e-03  -3.53520908e-03  -1.31677227e-03  -1.32251386e-03\n",
      "   -5.46654891e-04]\n",
      " [  3.81274854e-03  -3.22154939e-03   4.72847581e-03   4.43556642e-03\n",
      "   -5.66504663e-03]\n",
      " [ -5.71685424e-03   5.35010350e-03   7.12787658e-03  -6.62142327e-03\n",
      "   -2.82549110e-03]\n",
      " [  2.16451176e-03   3.32032063e-03   5.22359340e-03   6.39008467e-04\n",
      "    1.03332721e-03]\n",
      " [ -5.06724218e-05   6.04380589e-03   3.84108503e-03  -4.14315070e-03\n",
      "    1.64221107e-03]]\n",
      "output  layer  1  weights: \n",
      "[[-0.00561589 -0.00395673  0.00038798  0.0067894   0.00196047 -0.00396494\n",
      "   0.00662062]\n",
      " [-0.00528252  0.00650969  0.00582567 -0.00283006  0.00292162  0.00665145\n",
      "  -0.00451671]\n",
      " [ 0.00356357 -0.00644828 -0.00647517  0.00419943  0.0015961  -0.00056978\n",
      "   0.00363451]]\n",
      "first outputs:\n",
      "hidden  layer  0  output: \n",
      "[[ 0.49938732  0.50097451  0.50164149  0.50004075  0.49898787  0.49856267]]\n",
      "output  layer  1  output: \n",
      "[[ 0.49889376  0.49940307  0.49961307]]\n",
      "Training started ...\n",
      "Trainging finalized.\n",
      "After trainging new weights: \n",
      "hidden  layer  0  weights: \n",
      "[[-0.02596235 -1.60478541  2.2940128   2.94566008 -2.47259454]\n",
      " [ 0.16226753  1.17202477  0.5527764   1.50065807 -1.44523809]\n",
      " [ 1.76927038 -2.97753212  4.3401442   4.39040237 -1.13212188]\n",
      " [-0.09845956  1.02108795 -2.76669457 -4.14895526  4.13787622]\n",
      " [ 1.12645592 -1.97308028  3.22743016  3.43102583 -0.73893512]\n",
      " [ 0.01434889 -1.34995334  2.35963657  3.21426237 -3.00636604]]\n",
      "output  layer  1  weights: \n",
      "[[-1.84121218  0.65038177 -2.99950478  3.8469949  -2.45294802 -1.44840617\n",
      "   3.90447507]\n",
      " [-1.35932893 -2.22635202  5.74018093  4.31405161  2.98239025 -2.2825623\n",
      "   0.16400532]\n",
      " [ 4.16009142  1.28738961  3.45881404 -4.84658287  3.65803815  3.95042275\n",
      "  -1.71305257]]\n",
      "train validation\n",
      "validating ...\n",
      "from  130  data,  126  true,  4  false prediction.\n",
      "accuracy: 96.92%\n",
      "done !\n",
      "test validation\n",
      "validating ...\n",
      "Test index : 0\n",
      "Given answer :  [[ 0.00556929  0.3567643   0.73426659]]\n",
      "True answer :  [ 0.  0.  1.]\n",
      "Error:  [ 0.26573341]\n",
      "Test index : 1\n",
      "Given answer :  [[  9.76789616e-01   3.98560390e-02   1.86096246e-06]]\n",
      "True answer :  [ 1.  0.  0.]\n",
      "Error:  [ 0.02321038]\n",
      "Test index : 2\n",
      "Given answer :  [[ 0.04828774  0.81095675  0.04939269]]\n",
      "True answer :  [ 0.  1.  0.]\n",
      "Error:  [ 0.18904325]\n",
      "Test index : 3\n",
      "Given answer :  [[ 0.02559575  0.68229312  0.15359279]]\n",
      "True answer :  [ 0.  1.  0.]\n",
      "Error:  [ 0.31770688]\n",
      "Test index : 4\n",
      "Given answer :  [[ 0.037055    0.80239001  0.0762044 ]]\n",
      "True answer :  [ 0.  1.  0.]\n",
      "Error:  [ 0.19760999]\n",
      "Test index : 5\n",
      "Given answer :  [[ 0.00115732  0.0591299   0.98399729]]\n",
      "True answer :  [ 0.  0.  1.]\n",
      "Error:  [ 0.01600271]\n",
      "Test index : 6\n",
      "Given answer :  [[  9.73142950e-01   4.67086575e-02   2.22532348e-06]]\n",
      "True answer :  [ 1.  0.  0.]\n",
      "Error:  [ 0.02685705]\n",
      "Test index : 7\n",
      "Given answer :  [[ 0.00217054  0.12637732  0.94676999]]\n",
      "True answer :  [ 0.  0.  1.]\n",
      "Error:  [ 0.05323001]\n",
      "Test index : 8\n",
      "Given answer :  [[  9.67135016e-01   6.54842570e-02   2.72051174e-06]]\n",
      "True answer :  [ 1.  0.  0.]\n",
      "Error:  [ 0.03286498]\n",
      "Test index : 9\n",
      "Given answer :  [[ 0.04995125  0.80514136  0.04777233]]\n",
      "True answer :  [ 0.  1.  0.]\n",
      "Error:  [ 0.19485864]\n",
      "Test index : 10\n",
      "Given answer :  [[ 0.00362274  0.2228513   0.86823435]]\n",
      "True answer :  [ 0.  0.  1.]\n",
      "Error:  [ 0.13176565]\n",
      "Test index : 11\n",
      "Given answer :  [[ 0.00160737  0.10592123  0.96646452]]\n",
      "True answer :  [ 0.  0.  1.]\n",
      "Error:  [ 0.03353548]\n",
      "Test index : 12\n",
      "Given answer :  [[ 0.00872041  0.48013629  0.53824061]]\n",
      "True answer :  [ 0.  0.  1.]\n",
      "Error:  [ 0.46175939]\n",
      "Test index : 13\n",
      "Given answer :  [[ 0.02980089  0.71142886  0.12404005]]\n",
      "True answer :  [ 0.  1.  0.]\n",
      "Error:  [ 0.28857114]\n",
      "Test index : 14\n",
      "Given answer :  [[ 0.04983933  0.82643412  0.04606313]]\n",
      "True answer :  [ 0.  1.  0.]\n",
      "Error:  [ 0.17356588]\n",
      "Test index : 15\n",
      "Given answer :  [[ 0.0617966   0.81003077  0.03377109]]\n",
      "True answer :  [ 0.  1.  0.]\n",
      "Error:  [ 0.18996923]\n",
      "Test index : 16\n",
      "Given answer :  [[ 0.03674821  0.77212349  0.08428029]]\n",
      "True answer :  [ 0.  1.  0.]\n",
      "Error:  [ 0.22787651]\n",
      "Test index : 17\n",
      "Given answer :  [[ 0.00614539  0.36785907  0.70796488]]\n",
      "True answer :  [ 0.  0.  1.]\n",
      "Error:  [ 0.29203512]\n",
      "Test index : 18\n",
      "Given answer :  [[  9.70204253e-01   5.51858187e-02   2.50886999e-06]]\n",
      "True answer :  [ 1.  0.  0.]\n",
      "Error:  [ 0.02979575]\n",
      "Test index : 19\n",
      "Given answer :  [[ 0.01849808  0.64180171  0.24726966]]\n",
      "True answer :  [ 0.  1.  0.]\n",
      "Error:  [ 0.35819829]\n",
      "from  20  data,  20  true,  0  false prediction.\n",
      "accuracy: 100.00%\n",
      "done !\n",
      "Network Structure:\n",
      "hidden layer with  6 neuron\n",
      "output layer with  3 neuron\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "mlp = multiLayerPerseptron(train_data, train_labels, lr=0.1)\n",
    "mlp.add_layer( 6, layer_type='hidden')\n",
    "mlp.add_layer( 3, layer_type='output')\n",
    "mlp.fit(epochs)\n",
    "print \"train validation\"\n",
    "mlp.validate(train_data, train_labels)\n",
    "print \"test validation\"\n",
    "mlp.validate(test_data, test_labels, show=True)\n",
    "mlp.printNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs\n",
      "hidden  layer  0  output: \n",
      "[[ 0.30834117  0.32655853  0.00509803  0.67938374  0.01565695  0.33977707]]\n",
      "output  layer  1  output: \n",
      "[[ 0.00362274  0.2228513   0.86823435]]\n",
      "output layer costs: \n",
      "[[-0.00362274 -0.2228513   0.13176565]]\n",
      "output layer errors: \n",
      "[[  6.56213204e-06   2.48313516e-02   8.68109309e-03]]\n",
      "output layer error average: \n",
      "[[ 0.011173]]\n",
      "[ 0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "i =10\n",
    "\n",
    "mlp.forwardPass(np.array([test_data[i]]), np.array([test_labels[i]]), show=True)\n",
    "mlp.printErrors()\n",
    "print test_labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAEKCAYAAACMtLC3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e5ca38990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC6CAYAAABVwQ0gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADqhJREFUeJzt3X+w1XWdx/HnjR+VYRHRkMBt0cTEbVdJRVdSTm41YA3m\njG2649iwkzlMbL+mlt3WhsP+GKv9o0ZThmlFaWxwSpOhksid9SK5K0VwryIQUDJxuUgmiChtSd79\n4/O93ePhnHPPPZ9z7veeD8/HzHfu93u+H76fN4iv8+Hz/QWSJEmSJEmSJEmSJEmSJElta0zeBUij\nyApgNvBoHW3vAS4EHmllQVKjxuZdgDSKLB5G2/5sqeYV4GzgV1EVSQ16Td4FSAnryLsAnboMd6Vi\nEbCuZHsP8J2S7f3AXwLnAg8DzwG7gI+UtLkH+NeS7X8A+oBe4OOE0fhZJfsnAT8AXgAeL9k3MK3T\nAxzL+pictT2S9f0ohr8kDelMQnACTAX2Ab/Ots8CDgOnEUL+Y4SBzQXAs8CsrN3dwL9k6/OBg9m+\n1wP38upwvwf4LXAR4dzVvcCaknrKvwhuJczpj8mWuQ3/TqU6OHJXKp4mjJJnA1cAGwij7ncC84BN\nwIeydqsJ4dsNfI9Xj94H/A2wCtgJ/A5YVra/P/u1W4A/At8mfFlU8wfgDGBG1v6x4f32pOEx3JWS\njUABuDxb30gI9iuy9T8DLiGM8AeWvwWmVDjWGYRR/oDeCm0Olaz/DphQo7b/APYCPwZ+CSwd6jcj\nxTDclZKNwHsJ4d7FYNjPy7b3Z5+9uWQ5HfhkhWMdBDpLtjsrtBmOF4HPA+8AFgKfA66MPKZUleGu\nlAyE++sIUzI/IcydTwK2AT8EzgFuAMZly8WEk6wQTnAOnOT8DuEk7bmEufovlfU11MnQQ4QgH/BB\nwqWRHYQTsH/MFqklDHelZA9h3n1Ttv0CYQrkMcIc+THgA8B1wAHC6PxWYHzWvvTa9R8BtxFuUtoN\n/G/2+e8rtKXkswFFwtz+EcKc/kzCVTrHgP8B7iB8GUmjziTCX9bdhHnEiVXa7QOeIIycfjoilUnN\nNws4gQMinQK+SrgOGMLJoS9Xafc04YtAajfXAK8lzM2vI1wdIyVvF4NXGbwt267kaeAtI1KR1Fzr\ngecJNx09QOWraqTkHClZ7yjbLvUrwpTMFuCmVhclSRr6wWEPE0bl5f65bLvWQ5TmEk5cvTU73i4G\nT3gN6jy/n/09Q5QjSRpw/unQc6zylVtDhfv7a+w7RAj+Zwg3fPymSruD2c9ngQeBOVQK9/09cE+t\nh+xFerAI1xRbdvivfOzvW3bsh4ubeX/xkpYdH2Dp7Ntbd/CDRTij2LLD95/Z2ke0FHdCcdbQ7RrV\n8ZYW/r0H+HkRLiy27vgfb92hAfhmEW4qtuzw/f/eur8/xV9A8Z0tOzwd36++L+bM/zrCMzrIfq6t\n0OY0wk0iAG8gXIb2ZESfkqQ6xIT7lwkj+92EO+0GrpaZSrhZBMLIfhPhGR6bCU/F+3FEn5KkOsS8\nrOMw8L4Kn/cR7saDcDK11sOURs65hbwraNhZhWl5lxBnQiHvCqIUJuddQaQzCnlXEOfdhbwraFgh\nx+sET50bMmYV8q6gYe8oTM+7hDinF/KuIErhrXlXEGlqIe8K4lxYyLuChuU5MDh1wl2STiGGuyQl\nyHCXpAQZ7pKUIMNdkhJkuEtSggx3SUqQ4S5JCTLcJSlBhrskJchwl6QEGe6SlCDDXZISZLhLUoKa\nEe7zCe9F3QMsrdLmtmx/DzC7CX1KkmqIDfcxwDcIAX8ecD1Q/rbJq4CzgZnAJ4AVkX1KkoYQG+5z\ngL3APuBl4D7g6rI2C4HV2fpmYCIwJbJfSVINseE+Ddhfst2bfTZUmzZ/tZAkjW4x71AF6K+zXUdd\nv+7B4uD6uYW2fjWeJDVb12+h67n62saG+wGgs2S7kzAyr9VmevbZya4pRpYjSekqTH71e1mX767e\nNnZaZgvhROkMYDzwUWBdWZt1wI3Z+qXA88ChyH4lSTXEjtxPAEuADYQrZ+4CdgI3Z/tXAg8RrpjZ\nC7wELIrsU5I0hNhwB1ifLaVWlm0vaUI/kqQ6eYeqJCXIcJekBBnukpQgw12SEmS4S1KCDHdJSpDh\nLkkJMtwlKUGGuyQlyHCXpAQZ7pKUIMNdkhJkuEtSggx3SUqQ4S5JCWpGuM8HdgF7gKUV9heAo8C2\nbLmlCX1KkmqIfVnHGOAbwPsI70X9GeG1ejvL2m0EFkb2JUmqU+zIfQ7h9Xn7gJeB+4CrK7TriOxH\nkjQMsSP3acD+ku1e4JKyNv3AZUAPYXT/eWBHxaNdEFlNjpZ+6fa8S4iyrLuNv3//L+8C4vQvbeM/\ne6BjbX/eJUS5ed3X8y6hcR2fqborNtzr+a+6FegEjgMLgLXAORVbrigOrl9UgIsLcdVJUkL6uvbQ\n17W3rrax4X6AENwDOgmj91LHStbXA3cCk4DDJx1tcTGyHElK19TCTKYWZv5pe+vyDVXbxs65bwFm\nAjOA8cBHCSdUS01hcM59TrZ+crBLkpomduR+AlgCbCBcOXMX4UqZm7P9K4FrgcVZ2+PAdZF9SpKG\nEBvuEKZa1pd9trJk/Y5skSSNEO9QlaQEGe6SlCDDXZISZLhLUoIMd0lKkOEuSQky3CUpQYa7JCXI\ncJekBBnukpQgw12SEmS4S1KCDHdJSpDhLkkJMtwlKUGx4b4KOAQ8WaPNbcAewguyZ0f2J0mqQ2y4\n3w3Mr7H/KuBswqv4PgGsiOxPklSH2HDfBBypsX8hsDpb3wxMJLxTVZLUQq2ec58G7C/Z7gWmt7hP\nSTrlNeMdqkPpKNvur9pyRXFw/aICXFxoQTmS1J76uvbQ17W3rratDvcDQGfJ9vTss8oWF1tcjiS1\nr6mFmUwtzPzT9tblG6q2bfW0zDrgxmz9UuB5wtU1kqQWih25rwHmAZMJc+vLgHHZvpXAQ4QrZvYC\nLwGLIvuTJNUhNtyvr6PNksg+JEnD5B2qkpQgw12SEmS4S1KCDHdJSpDhLkkJMtwlKUGGuyQlyHCX\npAQZ7pKUIMNdkhJkuEtSggx3SUqQ4S5JCTLcJSlBhrskJagZ4b6K8HalJ6vsLwBHgW3ZcksT+pQk\n1dCMd6jeDdwOfKtGm43Awib0JUmqQzNG7puAI0O06WhCP5KkOo3EnHs/cBnQQ3in6nkj0KckndKa\nMS0zlK1AJ3AcWACsBc6p1HDZBwcH+IU3hqVd/Pfjf5V3CVGu/EPeFTSu46vL8i4hSv9jy/MuIUp/\nX3v/w7zjgf68S6jf9i54qquupiMR7sdK1tcDdwKTgMPlDYvTR6AaSWpX7yqEZcB3qw8MRmJaZgqD\nc+5zsvWTgl2S1DzNGLmvAeYBk4H9wDJgXLZvJXAtsBg4QZiaua4JfUqSamhGuF8/xP47skWSNEK8\nQ1WSEmS4S1KCDHdJSpDhLkkJMtwlKUGGuyQlyHCXpAQZ7pKUIMNdkhJkuEtSggx3SUqQ4S5JCTLc\nJSlBhrskJchwl6QExYZ7J/AI8BSwHfhUlXa3AXsIL8meHdmnJGkIsS/reBn4LNANTAB+DjwM7Cxp\ncxVwNjATuARYAVwa2a8kqYbYkfszhGAHeJEQ6lPL2iwEVmfrm4GJhPeqSpJapJlz7jMIUy6byz6f\nRni36oBeYHoT+5UklWnGO1QhTMncD3yaMIIv11G23V/pIMXewfXCG8MiScps74Knuupq2oxwHwc8\nANwLrK2w/wDhxOuA6dlnJyk6npek6t5VCMuA7y6v2jR2WqYDuAvYAXy9Spt1wI3Z+qXA88ChyH4l\nSTXEjtznAjcATwDbss++CLw9W18JPES4YmYv8BKwKLJPSdIQYsP9J9Q3+l8S2Y8kaRi8Q1WSEmS4\nS1KCDHdJSpDhLkkJMtwlKUGGuyQlyHCXpAQZ7pKUIMNdkhJkuEtSggx3SUqQ4S5JCTLcJSlBhrsk\nJchwl6QExYZ7J/AI8BSwHfhUhTYF4CjhZR7bgFsi+2xI1wt59Noc3V1H8y4hStev864g1r68C4jS\n1Zd3BXG6nsu7ggjbu3LrOjbcXwY+C/w54RV6nwRmVWi3EZidLf8W2WdD2jnce9q5eAz3vHUdzLuC\nOG0d7nW+zLoVYsP9GaA7W38R2AlMrdCuI7IfSdIwNHPOfQZhZL657PN+4DKgh/A+1fOa2KckqYUm\nAFuAD1fYdzpwWra+ANhd5RjdhC8CFxcXF5f6loGZk5YYB2wAPlNn+6eBSa0rR5IUqwP4FvC1Gm2m\nMDjnPod2PzslSW1gbOSvnwvcADxBuMwR4IvA27P1lcC1wGLgBHAcuC6yT0mSJEkpmg/sAvYAS3Ou\nZbhWAYeAJ/MupEH13OQ2mr2OcPVXN7ADuDXfchoyhvCv6u/nXUgD9jE4K/DTfEtpyETgfsIl4jsI\n9wKpScYAewmXaY4j/E9a6Sar0epywuWl7RrubwMuyNYnAL+gvf78YfBKr7HA48B7cqylEZ8Dvg2s\ny7uQBrT7xRergb/L1scCbxrJzlN/tswcQrjvI9xNex9wdZ4FDdMm4EjeRUSo9ya30ex49nM8YbBw\nOMdahms6cBXwn7TvjYTtWvebCIOzVdn2CcJjWEZM6uE+Ddhfst2bfaaRN4PKN7mNdq8hfEEdIkwx\n7ci3nGH5GvAF4JW8C2lQP/BfhHtobsq5luE6E3gWuBvYCnyTwX8FjojUw70/7wIEhCmZ+4FPE0bw\n7eQVwtTSdOAKwoPw2sGHgN8Q5qvbdfQ7lzAgWEB4btXl+ZYzLGOBdwN3Zj9fAv5xJAtIPdwPEE7q\nDegkjN41csYBDwD3AmtzriXGUeCHwEV5F1Kny4CFhHnrNcCVhHtS2snAI8+eBR4kTLO2i95s+Vm2\nfT8h5NUkY4FfEqYExtN+J1Qh1N6uJ1TrucltNJtMuOIB4PXAo8Bf51dOw+bRflfLnEZ4dAnAG4DH\ngA/kV05DHgXOydaLwFfyKyVNCwhXaewF/innWoZrDdAH/J5w7mBRvuUM23sI0xrdDD7Pf36uFQ3P\nXxDmS7sJl+R9Id9yGjaP9rta5kzCn3s34TLadvt/F+B8wsi9B/geI3y1jCRJkiRJkiRJkiRJkiRJ\nkiRJo8n/A1m6vIEh1zS7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e5a9a5810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.plot_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with another dataset\n",
    "dataset_file = \"/home/****/datasets/letters/letter-recognition.data\"\n",
    "\n",
    "def loadData(fileName):\n",
    "    \n",
    "    class_count = -1\n",
    "    same_class_dataCount = 0\n",
    "    data_class = \"\"\n",
    "    last_data_class = \"\"\n",
    "    input_model = []\n",
    "    classes = []\n",
    "    \n",
    "    fileI = open(fileName, 'r')\n",
    "    \n",
    "    input_s = fileI.readline()[:-1] # reading without readline\n",
    "    classes = set()\n",
    "    data = []\n",
    "    while input_s != '':\n",
    "        input_model = input_s.split(',')\n",
    "        for i in xrange(1,len(input_model)):\n",
    "            input_model[i] = float(input_model[i])    \n",
    "        data_class = input_model[0]\n",
    "        classes.add(data_class)\n",
    "        classes = list(classes)\n",
    "        input_model[0] = classes.index(data_class)\n",
    "        classes = set(classes)\n",
    "        \n",
    "        data.append(input_model)\n",
    "        input_s = fileI.readline()[:-1]\n",
    "        input_model = input_s.split(',')\n",
    "        \n",
    "    fileI.close()\n",
    "    return data, classes\n",
    "data, classes = loadData(dataset_file)\n",
    "\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.  15.\n",
      "  15.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "4000 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:12: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "## normalization\n",
    "data = np.array(data)\n",
    "max_v = data[:,1:].max( axis=0)\n",
    "min_v = data[:,1:].min( axis=0)\n",
    "print max_v, min_v\n",
    "\n",
    "normalized_data = (data[:,1:] - min_v) / (max_v - min_v)\n",
    "labels = data[:,0]\n",
    "\n",
    "cato_labels = np.zeros((len(labels),len(classes)))\n",
    "for i, label in enumerate(labels):\n",
    "    cato_labels[i][label] = 1.0\n",
    "\n",
    "split = int(len(normalized_data) / 5)\n",
    "print split, len(normalized_data)\n",
    "# test and train split\n",
    "test_data = normalized_data[:split]\n",
    "test_labels = cato_labels[:split]\n",
    "\n",
    "train_data = normalized_data[split:]\n",
    "train_labels = cato_labels[split:]\n",
    "#print train_data[:10], \"\\n\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer will close adding you can't add more layer.\n",
      "You can start training.\n",
      "first weigths:\n",
      "hidden  layer  0  weights: \n",
      "[[  5.81184323e-04   1.29297453e-03   4.33113636e-03   2.22882174e-03\n",
      "    2.99904080e-03  -3.18360688e-03   4.63108570e-03  -1.02824416e-03\n",
      "    2.50105766e-03  -4.49175525e-03   8.09137760e-04   3.63888508e-03\n",
      "   -4.04735412e-03  -3.77948341e-03   2.39026422e-03  -2.39737309e-03\n",
      "    1.51133888e-03]\n",
      " [ -6.63107723e-04   1.69920548e-03   3.19643801e-03   2.85087445e-03\n",
      "   -3.13516449e-04   4.58848459e-03  -5.68074952e-04  -4.12068336e-03\n",
      "   -1.65676353e-03   2.96360760e-03   2.40246504e-03  -2.94918002e-03\n",
      "    2.79304083e-03   4.54588492e-03   1.75082093e-03   2.40639026e-03\n",
      "   -4.70464196e-03]\n",
      " [ -3.87974375e-03   8.77992331e-04   1.93562474e-03   1.61412495e-03\n",
      "    4.45166447e-03  -2.31337555e-03  -4.00322559e-03   4.24435472e-03\n",
      "   -2.14955926e-03  -4.70089899e-03  -3.71072868e-03  -2.42706279e-04\n",
      "   -2.89761352e-03   2.39828053e-03   9.46413779e-04   1.23161105e-03\n",
      "    1.40963511e-03]\n",
      " [  2.78285965e-03  -2.21269563e-03   3.95623423e-03   2.76744160e-03\n",
      "    4.62297181e-03  -3.30914528e-03   1.96067170e-03   3.87259695e-03\n",
      "    2.20149962e-03   3.68080777e-03   9.37232584e-04  -1.51703877e-04\n",
      "   -9.84425936e-05  -2.93723703e-03  -4.52980864e-03  -1.24835521e-03\n",
      "   -1.67019012e-03]\n",
      " [ -2.75744217e-03   4.74786257e-04  -8.59076096e-04   3.08507950e-03\n",
      "    2.58641854e-03  -2.73707801e-03  -2.84242831e-03  -2.58185556e-03\n",
      "    1.01586909e-03  -1.67637735e-03  -3.76929980e-03  -4.11632910e-04\n",
      "   -2.47820838e-03   4.59301289e-03   3.99171528e-03  -5.92617680e-04\n",
      "   -4.04370844e-03]\n",
      " [  3.85195492e-03   2.48714700e-03   4.27945845e-03  -4.23806020e-03\n",
      "   -1.71698775e-03   4.20878278e-03   4.08515699e-03  -3.62045817e-04\n",
      "   -7.04721348e-04   2.60494521e-03  -2.43520172e-03   2.84671430e-03\n",
      "   -2.54613357e-03   2.61585356e-04  -6.00144008e-04  -1.15575217e-03\n",
      "    1.76743826e-03]\n",
      " [  4.17662849e-03   2.95365613e-03   6.40339762e-04  -1.69326600e-03\n",
      "    4.41428756e-03   1.30137158e-03   5.23859512e-04   1.21072160e-03\n",
      "    3.25876177e-03  -1.93722398e-03   3.25194759e-03  -9.27045754e-04\n",
      "    1.53884554e-03   1.24194986e-03   2.27866597e-03  -3.69798891e-03\n",
      "    7.97341965e-04]\n",
      " [ -1.06836049e-03  -2.19858717e-03  -3.08624757e-03   9.23124711e-04\n",
      "   -3.48426575e-03  -2.55423146e-03   3.05540466e-04   3.98644293e-03\n",
      "    4.42721485e-03   3.08975794e-03  -3.63014373e-03  -3.54502746e-03\n",
      "   -2.42026085e-03   9.90442666e-04   2.13244423e-05  -4.50973285e-03\n",
      "   -4.44164873e-04]\n",
      " [  3.89857054e-03  -4.10532071e-04  -2.21087690e-03   2.40330921e-03\n",
      "   -3.96736765e-03  -1.12500729e-04  -2.22564512e-03  -2.79350799e-03\n",
      "    4.62511321e-03  -3.51475559e-03  -2.74275711e-03  -3.66587154e-03\n",
      "    1.29108534e-03   4.00925306e-03   4.32102487e-04  -3.89948409e-03\n",
      "   -2.43382276e-03]\n",
      " [ -2.37506875e-03  -4.58222060e-03  -3.66585670e-03   2.90703464e-03\n",
      "    2.79840463e-03   1.08663662e-03  -3.20659290e-03   3.47792069e-03\n",
      "   -1.97910163e-03  -3.36870591e-03   5.89237457e-05   3.93040411e-03\n",
      "   -1.89163369e-03  -3.32146147e-05   5.97093702e-04   3.90314414e-03\n",
      "    9.91094990e-04]]\n",
      "output  layer  1  weights: \n",
      "[[ -1.55728087e-03  -3.32096730e-03   3.92462229e-03   7.94552760e-04\n",
      "   -4.55905309e-04   1.27452633e-03  -4.03381275e-04  -2.95561051e-03\n",
      "    3.18502964e-03  -9.58900234e-04   1.29046416e-03]\n",
      " [  3.42289146e-03   1.26804062e-03   1.22926126e-03  -7.30021838e-04\n",
      "    7.38620729e-04   1.21078618e-03   1.24733966e-03  -1.26412691e-03\n",
      "   -2.80642725e-03  -2.58566572e-03   2.25958063e-03]\n",
      " [ -2.75269123e-03   3.53749093e-04  -3.99106717e-03   1.02989621e-03\n",
      "    1.69212699e-04  -8.04811958e-04   1.21698504e-03  -4.00402241e-03\n",
      "   -2.23955924e-03   1.03521091e-03  -1.50605971e-03]\n",
      " [ -1.59589415e-03  -3.46646866e-03   1.29388471e-03  -2.47586419e-03\n",
      "    1.45021315e-03   2.88463315e-03  -2.44132501e-03   3.89158711e-04\n",
      "   -2.19212932e-03   1.80067315e-03   6.54381271e-04]\n",
      " [  6.38594502e-04   2.12674102e-03   1.10272513e-03  -1.45247647e-03\n",
      "   -1.69408470e-04   2.92259953e-04   2.25333148e-03   8.19524415e-04\n",
      "   -1.51509260e-03   3.59580407e-03   1.61340773e-03]\n",
      " [ -2.53279288e-03   8.34979951e-04  -2.22444159e-03   1.86170574e-03\n",
      "    8.43713665e-04  -2.19506739e-03  -3.97890052e-03   3.01819694e-03\n",
      "    1.76712635e-03   3.63849283e-04  -3.99618932e-04]\n",
      " [ -2.49220102e-03   1.29609650e-03   3.09887020e-03  -3.57474701e-04\n",
      "    3.12172402e-03   1.65109693e-03  -2.42569441e-03   2.50189549e-03\n",
      "   -2.20298673e-03  -2.51808198e-03   1.08215026e-03]\n",
      " [  6.20322055e-04  -2.42009462e-03  -3.97726709e-03   1.78168256e-03\n",
      "   -1.89791963e-03   1.11959538e-04  -2.39514161e-03   3.14125372e-03\n",
      "    1.92675839e-03  -3.42173472e-03  -2.59945364e-03]\n",
      " [  1.26867105e-03   1.46069923e-03   2.48645269e-03   1.55506655e-03\n",
      "   -2.56265436e-04   1.94788350e-03   3.48863112e-03   5.64741231e-04\n",
      "    6.98814635e-04   2.36225876e-03  -3.82873541e-03]\n",
      " [  3.06535073e-03  -1.73555166e-04   2.53915656e-03   3.33097655e-03\n",
      "   -1.29072956e-03  -1.30521638e-03   3.36405850e-03  -3.86703203e-03\n",
      "    1.24269842e-03  -5.37978110e-04  -1.05742478e-03]\n",
      " [ -3.48020035e-05  -2.24366832e-03  -3.55610989e-03   1.62904421e-03\n",
      "    3.62663305e-03  -3.63330038e-03  -2.36975322e-04  -1.24890683e-03\n",
      "   -1.40368304e-03   3.13244753e-03   9.89293018e-04]\n",
      " [ -2.00304454e-03   3.32056924e-03   1.36344912e-03  -1.90646311e-03\n",
      "    3.29581976e-03  -3.56538772e-03   1.92103448e-03   7.36476866e-05\n",
      "   -9.01638404e-04   8.20658942e-04  -1.87778099e-03]\n",
      " [  3.92745605e-03   2.28048723e-03  -2.50068652e-03  -1.92203545e-03\n",
      "   -2.85272841e-03  -1.08659153e-03  -7.67370524e-04  -3.78190140e-03\n",
      "    2.21368646e-03   2.26773420e-03  -6.57122689e-05]\n",
      " [  1.14803826e-03   1.17395790e-03  -1.82344137e-03   1.42347193e-03\n",
      "    3.80677252e-03   2.69290946e-03   1.43345298e-03  -7.83253305e-04\n",
      "   -3.35836636e-03   4.96119828e-04  -3.61776643e-03]\n",
      " [  1.00000103e-03   8.14028780e-05  -9.45356619e-04   2.37831398e-03\n",
      "   -1.05469395e-03  -1.78716274e-03  -3.78723995e-03   3.97836986e-03\n",
      "    9.07862371e-04   2.55379521e-03  -3.34277879e-03]\n",
      " [  1.79536637e-03  -1.74459326e-03   2.31200637e-03  -3.24871298e-03\n",
      "   -3.06239607e-03   3.68874693e-03   1.78162688e-03  -2.32758863e-03\n",
      "    2.08034112e-03   3.80549996e-04  -1.07420363e-03]\n",
      " [  2.44465905e-03   2.99470346e-03   1.58176277e-03  -1.18282720e-03\n",
      "    8.11918150e-04   1.94214334e-03   3.22246996e-03   1.28849819e-04\n",
      "   -3.21186651e-03  -7.17905611e-04  -9.68916668e-04]\n",
      " [ -1.87895685e-03  -1.92330382e-03  -4.01189380e-03   2.51435598e-03\n",
      "    1.02253691e-03  -2.18864995e-03   1.83678307e-03  -3.05196981e-03\n",
      "   -2.92536162e-03   7.01389978e-04  -2.65124348e-03]\n",
      " [ -1.93746034e-03   7.65541183e-04   6.50457616e-04  -7.03475415e-04\n",
      "    3.88808702e-03  -1.85476167e-03  -3.48617856e-04  -2.60245603e-03\n",
      "    3.36009473e-03   2.83535414e-03   3.36388204e-03]\n",
      " [ -4.30894873e-04  -2.83025415e-03   1.06840346e-03   1.86829867e-03\n",
      "    2.10893405e-03  -1.22641955e-03   2.64064190e-03  -3.32550736e-03\n",
      "   -5.22778648e-04   3.75800702e-03   1.19151363e-04]\n",
      " [  4.30914703e-04  -1.61323097e-04   5.99983170e-05  -5.56865388e-05\n",
      "   -3.08419370e-04  -7.80487235e-04  -2.18471767e-03  -2.72694011e-03\n",
      "    1.05313506e-03  -1.68957049e-03   3.16349763e-03]\n",
      " [ -3.23352342e-04   9.63716585e-04  -2.55515504e-03  -4.02059723e-03\n",
      "    1.24393150e-04   1.37021698e-03  -2.66940332e-05   1.38745564e-03\n",
      "    3.91918668e-03   3.95393985e-03   7.06219844e-04]\n",
      " [  3.45317118e-03  -2.39069111e-03  -1.01868657e-03  -1.56952542e-03\n",
      "    3.30434536e-03   2.00722573e-03  -1.24899196e-03  -3.10533898e-03\n",
      "   -2.55311616e-03  -1.12419610e-03  -3.93979686e-03]\n",
      " [ -1.00358379e-03  -1.71663892e-03   3.93874066e-03  -3.69836049e-03\n",
      "   -2.01136356e-03  -3.26565904e-03  -2.56520808e-03   2.08503595e-03\n",
      "    2.68567912e-03  -3.11162455e-03   2.38742022e-03]\n",
      " [ -1.00079226e-03   1.32179275e-03  -1.37748235e-03   3.23800260e-03\n",
      "    1.14983080e-05  -3.75821269e-03  -2.47856452e-03   1.87578512e-03\n",
      "    3.86316088e-03   1.42133886e-03  -2.27070089e-03]\n",
      " [  1.80692883e-03  -3.63949796e-03   2.10601967e-03  -6.13121966e-05\n",
      "    3.29642860e-03  -2.24846064e-03  -3.55607896e-04   1.41740654e-03\n",
      "   -3.00324625e-03   1.14706926e-03   1.94659139e-03]]\n",
      "first outputs:\n",
      "hidden  layer  0  output: \n",
      "[[ 0.49979324  0.49913748  0.50105104  0.49965367  0.50178647  0.497625\n",
      "   0.49925616  0.50103265  0.50196502  0.50004187]]\n",
      "output  layer  1  output: \n",
      "[[ 0.49973488  0.49922119  0.50162776  0.50038073  0.49863639  0.50037683\n",
      "   0.49951921  0.50146559  0.49901133  0.49946895  0.50024659  0.50016439\n",
      "   0.50029571  0.50013116  0.50041776  0.50006392  0.49924369  0.5019025\n",
      "   0.49864836  0.4995808   0.50000395  0.49922325  0.50151702  0.5004809\n",
      "   0.50017389  0.49945211]]\n",
      "Training started ...\n",
      "Epoch :9"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "mlp = multiLayerPerseptron(train_data, train_labels, lr=0.1)\n",
    "mlp.add_layer( 10, layer_type='hidden')\n",
    "mlp.add_layer( len(classes), layer_type='output')\n",
    "mlp.fit(epochs)\n",
    "print \"train validation\"\n",
    "mlp.validate(train_data, train_labels)\n",
    "print \"test validation\"\n",
    "mlp.validate(test_data, test_labels)\n",
    "mlp.printNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANoAAAEKCAYAAAB0VEtAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzeEJDTLI3KwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e5ca49350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANMAAAEKCAYAAACIQWCKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFtdJREFUeJzt3XmYFNW5x/FvZ4JEFkFA2TIBBEUUBR65kAhIu4FIBL2u\neCWIqCQhxhujokRDE29EYvTmaiLGsIgoRCNKADfEOAioeCGAiLIGEpQdRTEakKHyR9VszHDemp7T\nMz3M7/M8/dDV7zunzrTzWt11qs4BEREREREREREREZEaJaeqOyDlMh7oArwRI/dx4Azg9Ux2SIp8\nvao7IOXyg3LkBtHjcA4C7YC/VahHUuhrVd0BqVKJqu7AkUTFVHmGArOKba8Dnim2vRk4HTgZeBXY\nDawGLi+W8zhwT7Ht24EtwIfA9YRHmxOKxRsBc4DPgLeLxQo+Jq4A9kb7aBLlfhLt+w1UbJKl2hD+\noQK0ADYB/4i2TwA+BuoQFtUQwv/RdQZ2Ah2ivMnAL6LnFwBbo9jRwJOULKbHgV1AV8Lvxk8C04v1\n59DCG0v4nSwnevRI+zetoXRkqjwbCY8CXYCzgFcIjyrtgd7AAuC7Ud4Uwj/25cBzlDw6FbgCmAR8\nAHwJjD4kHkQ/uwTIB54iLM7D2Q80B1pH+YvK9+uJiqlyzQeSQK/o+XzCQjoret4K6E54BCt4XA00\nLaOt5oRHsQIflpGzvdjzL4F6jr7dD6wH5gIbgJHWLyMlqZgq13zgbMJiyqOouHpH25uj144t9qgP\njCijra1AbrHt3DJyyuNz4FagLTAAuAU4p4Jt1igqpspVUEzfIPyIt5Dwu08jYBnwAnAScA1QK3r8\nB+FJCQhPCBScFHiG8KTGyYTfte4+ZF/WyYPthIVToD/hqfIE4QmL/OghMamYKtc6wu9NC6Ltzwg/\nUi0i/I6zF+gDXAV8RHj0GQscFeUXHzt6GXiIcFB2LfBW9Pq+MnIp9lqBFOF3s08Iv5OdSHgWcS/w\nJvA7wuIXqXE6AAfQ/yBF0nIJUJvwu9UswrN3IpKGl4A9hIOsMyj7rJ+ISM1xAeHlLuvQmIRI2nII\nB/haE56+XU7RJS+h+r0LzibpoceR8TizR4BDurdgdCMspk3R9h+BgYSXtoT2zodvH7LvzSnITRVt\nT3DvJJgW4zrL3XZK4nLne8CCc88o9dqk1BauS7Uo3N5PbXM/z3OxMz6A2WYb5/9soZlD95KbqemQ\nGlS0nRjo/n0BBgWTzJzpH13tjK/65tFmG6dsL7mduh9Stx2SdK67jUkrB7kTgGGJu8wcOMYZrbWr\nKJ4/biw5I+8sEf+qSQPnz6d7GrUlpS9laZlmWyJHhHSLyf5fn0gNk+7HvI8ofV1Y6QstN6eKnh+T\nDB/VRJdk/aruQmzJjlXdg/iSZ1Z1D+JJ9OjJwYULCBbF+NgdSbeYlhBeftKa8BqzK4HSH2yLfz+q\nZqpVMZ1W1T2IL1lN7pL6Ws9e4ZOCf4GD99/n/Jl0i+kA8CPCe3JygIkUP/kgUgNVZEKVl6KHiJDh\n2YnGvXWTMz5yxcPOeOIvMc5zPGqnDOw03RmfzQCzDeu0N8CVPO2Md2a52ca1v3zEzNlNE2f80wNH\nOeMADe7eb+Z0uOevzvg/gt5mG3869Dx+GR5dOdwZ3/Z3+1atvKCnmbOU0kMgxV3Ks854a6N9XWEs\n4omKScQTFZOIJyomEU9UTCKeqJhEPFExiXiS0XGmlZzujPfp9Gdn/M/z7LGdR+raC0M0ZI8zPuz3\n08w26l2z08z5vK77EqQF9HLGAe5gnJnTYdEmZ/y+HivNNmIMm3Etk53xfhPzzDbqXWW/b+fWneeM\nv99qr9nGZK41c5oY9+ssp4vRwlxnVEcmEU9UTCKeqJhEPFExiXiiYhLxRMUk4omKScQTFZOIJxkd\ntG3Hemf8yRducMaPPvmguY8T2r5v5gw1Bh97Dn/VbGPhqvPNnOSpec649X4APMH3zJyBPdw3O36P\nJ8w2xr78CzPnyzPqOOODhtlz7+WWmBGubNZgtjXYCnBW4So9hzeP85zx9qwx23DRkUnEExWTiCcq\nJhFPVEwinqiYRDxRMYl4omIS8STGAkhpC7oFec6Ei5lZ4Z18gb1G0LeMsY4b50812/hm73VmzkXG\n+kv1sW9yO5vXzZx+r+Q54737vmy2sR97osrLjEkZ/5efmG00jjFG1JqNZo6lK0vNnH3G77yHhs74\nbxMjwVEzOjKJeKJiEvFExSTiiYpJxBMVk4gnKiYRT1RMIp6omEQ8qejNgZuAz4B84CugW/Fgc7Y4\nf/hFLnTG4wzItmCrmWMNxnXq/bbZRpxBTqu/V/OU2Ya1KiDYg7LWYCvATRMnmDkMc4etQWqw3/s4\nXidp5sT5O8gnxxm33rffGu1XtJgCIAl8XMF2RKo9Hx/zMnlJkki1UdFiCoB5wBLAPaGDyBGuoh/z\negBbgeOAV4HVEGNmC5EjUEWLqeBb307gecITEIXFtDpV9IWuSfIUmiRPqeDuRCrPirw9rMj7NHZ+\nRYqpDpAD7AXqAn2AMcUTTk5dVoHmRapWp2RDOiWLzkY+OcZ9K09Fiqkp4dGooJ2nsFaDEjmCVaSY\nNgKdfXVEpLrL6IyuP+NeZ/wG/uCML/v4O+Y+rmr0uJnzNFc54zsXf8ts467uo8ycd+jujH933wtm\nG58ubGbmcMAd/q++9uBw0NEe0fgVP3LGR0582Gyj2bC/mTn5xp9hF5aZbVyI/d5afwfWoK5FlxOJ\neKJiEvFExSTiiYpJxBMVk4gnKiYRT1RMIp5kdEbXYK2x85NWOONbg7PMnTR/bY+Z0/Nc98qAC0b2\nMduYM+4cM6ch7r68SH+zjbGJW82c14ILnPEJXG+28SC3mDlPc6Uz3sK4+ROgb/4rZs7ZOX9xxpe+\n515ZEODjjt8wcxon7nLGOwbuv4P3Et1AM7qKZJ6KScQTFZOIJyomEU9UTCKeqJhEPFExiXiiYhLx\nJKODtp2Ct5wJ1sDhZIaaO2nIJ2bODpo648/vvsRso29je/BxE62d8cE8YbbRnXfMnOQG9wy0321r\nz+ham31mzkbj94nT1w20NXOsmX/zONtsIxlj+dLa7HfGc43lWu9OPAAatBXJPBWTiCcqJhFPVEwi\nnqiYRDxRMYl4omIS8UTFJOJJRmd0bcd6Z/wJBjvjM/95sbmPi+raS0F+QR1nPM6A7PvYK3h0Z7Ez\nbg0aQrxBzt+0/b4zvpLTzDYmvuCerRWgY///d8abst1sI87ypTnkO+NHxRhgfo3zzJyrmeaMW38n\nFh2ZRDxRMYl4omIS8UTFJOKJiknEExWTiCcqJhFP4owzTQL6AzugcACjEfA00ArYBFwBpacznXH+\nNe6W3cMlBAftexenXH6FmXPtiU874wPXTTfbWP9xRzNnaKNHnPH67DXbGPaUeywEgDx3eMgfxptN\nBB/a7+0YbnfGR9/7K7ONRI/AzDmn9xxnvCtLzTamzR1m9+Vhd19mzL7QbMMlzpFpMnDofLx3AK8C\nJwGvRdsiNVqcYloApe4NHwBMiZ5PAexLFUSOcOl+Z2oKhdeSbI+2RWo0HycggughUqOle6HrdqAZ\nsA1oTnhyorQNqaLnxyahUTLN3YlUvvfydrMq7+PY+ekW0yxgCDAu+ndmmVltU2k2L1L1OiYb0zHZ\nuHD7mTHuuyDifMybDrwJtAc2A0OB+4DzgbXAOdG2SI0W58g06DCv2zeQiNQgGZ3RFXY5EzbTxBnP\nfdQ+rxFst3+Fo0Z86ox/te0Ys42Jp15t5gxb7B5wTXUfabYxeqg9EPry5N7O+FX7/mi20av2AjNn\nzvzLnfHgBvu9H7z2MTNn6vIbnfFEfozzW/9tpwT93P29e9Sdzvj/JMaCZnQVyTwVk4gnKiYRT1RM\nIp6omEQ8UTGJeKJiEvEko5NQsryxM5zb2T0p4+3Dx5i7eI5+Zs7+uQ2c8fl9upltJHu5J5gEexwj\nxpyNrJp8gpnTsPR9mCV8mmpmtjHnvr+ZOWuDXHfCyWYTTD3BPYYEgHs4i+Dr9njW/AX2f8NEB2O8\nqrvVwlhnVEcmEU9UTCKeqJhEPFExiXiiYhLxRMUk4omKScQTFZOIJxm9ObBnMNeZ0Bf3in1xVsCL\nM0vqAXKc8Vfoa7ZxOisrvB/r9wVowyYz5/v5jzrjF+a8aLYR5721Bodz2Wy2sYPjK7yfZXQ22+iP\n/TvvMm5G7cIyZ/ynifGgmwNFMk/FJOKJiknEExWTiCcqJhFPVEwinqiYRDxRMYl4ktE7bfONQcy7\nNj7gjJ/W5h1zH51ZXq4+pev4wyz0Udw8znXG91PbbGMMo82cvXvqO+PTDtizzz7X9D/NnNtwzy57\nHvPMNnI4YObUxn3HdZyB+TW0N3Pas8YZf55LjBbcy5vqyCTiiYpJxBMVk4gnKiYRT1RMIp6omEQ8\nUTGJeBJnnGkS0J9wRfWCO8pSwPXAzmj7TuDl8u58cBv3qnK9sFe3+4Kjy7vbUuLckLcZY3ZToIsx\n5jWc35tt7MY9Cy5A18ZLnPHu2LPPXrzUvlGx0xlvO+Nxxna20NzMacFWo40WZhtNY4wDWu38mIec\n8YVG+3GOTJOBCw55LQAeBLpEj3IXksiRJk4xLQA+KeP1TN7yLlLtVOQ7003ACmAi0NBPd0Sqr3SL\naTzQBugMbAXcF9mJ1ADpXuha/NveBGB2WUmbU1MKnx+T7ESDpD3LjEi2WJW3i1V5u2Pnp1tMzaHw\nFMwlUPY8WLmpIWk2L1L1Tk024dRk0fRgz45Z58yPU0zTgd5AE2AzMBpIEn7EC4CNwPC0eityBIlT\nTIPKeG2S746IVHcZndH1naCjM2GPcRKwQ8IaJoMFwUAzZzYDnPHpp11ntvHwyuvNnNs+dd9M969d\nx5ptvNT2bDOn36A8Z7zV9NVmG5sGdDBzRs262xkfm/sLs40Y9w9yTvs5zviX1DHbaMEWM2fGR5e5\n4y3dN0xemngJNKOrSOapmEQ8UTGJeKJiEvFExSTiiYpJxBMVk4gnGZ2EcjJDnfHx829xxoOV9jDY\nSk43c3rxhjM+faa7nwC/ZJSZ82AD9+9zSoMPzDYmYI9ncZc7HOcmxMTNgZnzDBe5E35jNsFx7f9h\n5tThS2f8WGNlQYBuMW6I3NXSvXKgtYKhRUcmEU9UTCKeqJhEPFExiXiiYhLxRMUk4omKScQTFZOI\nJxkdtF1MN2d8UG/3Dbtv08ncxym8b+Ysx5jIZb09OHxsW3tALw/3jX3xVg78uZkzu517MHUmF5tt\nNOi5zcyZZdxU2e3S+WYbX8S4sc8aVH/cGPwHWEJXM6czy5zxlYUTFh/OW86ojkwinqiYRDxRMYl4\nomIS8UTFJOKJiknEExWTiCcqJhFPMjqj68ygjzNhYN+5znhi7mh7L3NSZspb/d2DtqMZY7axJN8e\nFMzJyXfGN+5rY7bxeu2kmXPRmtec8aC+/Z81Md6+05bv/8sZ/kHLR8wm4gyq37Rmgjvhc7MJgt0x\n/pTdq5fy3Kh+zrhmdBWpJComEU9UTCKeqJhEPFExiXiiYhLxRMUk4ol1c2Au8ARwPOH6tY8BDwGN\ngKeBVsAm4AooPR3mwL+4x5Gav7LBGR/Gi0b3YOLvzBRzps4/5V9utvFujnXjmD0zab0fucehAJhg\nj8u0CtwrA/7rn/ZugloxxmXcC/qRWGiPVbWaaq9iiDHMFNwWo69T7ZTAuO9yN43tRhysI9NXwE+A\nU4FvAyOADsAdwKvAScBr0bZIjWYV0zZgefT8c+ADoCUwAJgSvT4FYtwnLXKEK893ptZAF2Ax0BTY\nHr2+PdoWqdHiFlM9YAZwM7D3kFgQPURqtDizE9UiLKSpwMzote1AM8KPgc2BHWX9YGpK0fNkJ0ga\nkwSJZJM1edtYk7fdToxYxZQAJgLvU3I1nlnAEGBc9O/M0j8KqSGx+yGSddonm9E+2axwe86Ylc58\nq5h6ANcA70LhpGN3AvcBzwDDKDo1LlKjWcW0kMN/rzrPc19EqrWMzujKAXd4eKKtMz7mvRhLRY4w\nlooEOqzZ5Iy/094ekO05+q9mTmKt0d8/2p+/g1+fauZwtTt867R7zCbaj77bzLkx5f59jptqL7F5\nGu6PRgBd73fftZdYFOP81v8tNVOCf7pv8PziD/bssy66nEjEExWTiCcqJhFPVEwinqiYRDxRMYl4\nomIS8UTFJOJJZgdtW7jD7ayf32Tv4oo9s82cfj2ec8bzyTHb+OzntezOPGrE77DvVEk8ag9QTp12\nmTM+il+abTSaZqZwYzd3X3bO+JbZRs6li8ycGYuvsTtjCJbZM+7ygTt8Jm9WqA86Mol4omIS8UTF\nJOKJiknEExWTiCcqJhFPVEwinmR2nKmBEQ8udcd/H2Mf2+yUH/ZwT/t62j77Brbrak8yc+4c4Z4y\ndCqDzTY+nHeimdOCLc74fmqbbdw66GdmzgOMcMZ/+qy9cmA73LP2AvChOzzu0pvMJnZSz8w5rrF7\nCcI3OdNo4T1nVEcmEU9UTCKeqJhEPFExiXiiYhLxRMUk4omKScQTFZOIJzHWN0xbMCRwD+rNzh/g\njJ+d87q5k6ONpS8BGvKJM76HY802lmMv4XExzxv7aWi2scG+ZZLOhevPlW0Trc02pv/dHkC+vdW9\nzvgCeplt5GAvPVqHL5zxXDabbeylvpljzS67HvcMw1MSPwRHzejIJOKJiknEExWTiCcqJhFPVEwi\nnqiYRDxRMYl4Yo0z5QJPAMcDAfAY8BCQAq4HdkZ5dwIvH/KzwWOBe3LBG38w1RkP6trDYIN//ZiZ\nMwL3zYFn7X7DbOOr1ceYOcEBo78nmE2QuDfGKnnGUFQw2H7fEvfY+wly3e0kWsXoa9cYOV93LzH5\nZZOjzCa+YUwwCdC9a54zvnhH0hlPhHOIHvZNse60/Qr4CbAcqAcsBV4lLKwHo4eIYBfTNopuDP+c\ncILZltF2Jq+eEKl2yvOdqTXQBXg72r4JWAFMhBjXyYgc4eIWUz3gWeBmwiPUeKAN0BnYCjyQkd6J\nVCNxZieqBcwAngRmRq/tKBafAJS5FMWs1IrC5+2TTaN/m6XRzcp3cOECvtbTvpAzK6zPg3bJqu5F\nPG/nwbeTVd0LU160eEdeORbGsIopQfgx7n3gN8Veb054RAK4BMq+HHdAqlOJ7VmpFdWmmIJFC6G6\nFNOGvOpTTIvzqkcxvQmp2yDZo+i1Mb92/4xVTD2Aa4B3gWXRa6OAQYQf8QJgIzA8nQ6LHEmsYlpI\n2d+rXspAX0SqtUye3s4DemewfZHKNh9IVnUnREREROSIdAGwGlgHjKzivlg2UXT28p2q7Uopk4Dt\nlByKaER4veRaYC7ZczVKWX1NEa55sSx6XFD53SpTLvA6sIpwqYsfR69n3XubA6wnvBypFuFFsx2q\nskOGjYRvYjbqRXhJV/E/0F8Bt0fPRwL3VXanDqOsvo4Gbqma7jg1g8Lpp+oBawj/RrPuvf0OJW/P\nuCN6ZKuNQOOq7oRDa0r+ga4GmkbPm0Xb2aI1pYvpp1XTlXKZCZxHOd/byrg5sCWUmPjsQ4quPM9G\nATAPWALcUMV9iaMp4ccpon+bOnKzQbZfIN2a8Ii6mHK+t5VRTDHuDssqPQjfzH7ACIgx02L2CMju\n9zvbL5CuR3gd6s3A3kNi5ntbGcX0EeEXvAK5mAsvVqmCaw53As8D3aqwL3FsJ/wIAuE1kzscuVVt\nB0V/lBPIrve24ILuqRRd0F2u97YyimkJcCLh4fMo4EpgViXsNx11oHCe3bpAHw5zEW8WmQUMiZ4P\noegPIRs1L/b8sBdIV4HDXdCdle9tP8IzJOsJ54vIVm0IzzYuJzxFmm19nQ5sAfYTfg8dSnjmcR5Z\ndPo2cmhfryOcT+Rdwu9MM8me73c9gYOE/92Ln7bP1vdWREREREREREREREREREQO9W8WLGxzzMBI\n+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e885d9bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.plot_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
